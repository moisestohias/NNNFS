[Lecture 4 - Automatic Differentiation](youtu.be/56WUlMEeAuA)
Transcript:
- Okay, hello everyone. Welcome back to Deep Learning Systems: Algorithms and Implementation. My name is Tianqi Chen. You can call me TQ. It's great to bring this class with Zico together to everybody. And hopefully we enjoy the journey of building our deep learning frameworks from scratch. So today we're going to cover a very important part of our entire lecture sequences.
We're going to tell you a bit about how do we do automatic differentiation. So first of all, let's start to recap on why do we need to do automatic differentiation and different differentiation methods. Then the second part of lecture, we're going to cover a main method called reverse mode automatic differentiation that is being used in modern deep learning frameworks.
So let's start with the first one. So how does automatic differentiation fit into the general machine learning workflow? If we recall previous two classes where I talk about the basic elements of machine learning or you think about machine learning algorithm. Usually you want to think about that in a few perspectives.
First of all, we need to think about hypothesis class, that is for given input x, how do you get predictions for the given input. For normal cases, in the last few lectures we talk about multi-layer perceptron and the single-layer perceptron algorithms. In the following lecture, we're going to talk about different hypothesis class like convolutional neural networks, transformers, and generative neural networks that give us for a given input that transform a given input to the prediction.
Of course, for given hypothesis class, there are still several things that being undetermined specifically each hypothesis will be parametrized by a set of parameters theta. And we will need to learn a good theta from training data. So the second element in machine learning usually that you will see is a loss function.
The loss function defines for a given hypothesis class and a parameter theta how good our prediction is. And usually we measure it in terms of training dataset. So we'll effectively try to find a set of theta that minimizes the loss function on a training dataset. And then we'll use that learn parameter theta to run prediction on future datasets we see.
So loss function is the element that measures how good we are. Finally for given loss function, we'll still need to be able to find a good theta that minimizes that loss function. That's where the third perspective coming in, which is optimization method. Most of the machine learning optimization algorithms actually in terms of deep learning usually takes this form of stochastic gradient optimization.
So effectively we will try to take a loss function and try to compute a gradient of a loss function with respect to the parameter theta. And that gradient function is a direction that help us to, usually it increases the loss function by a tiny bit, and our goal is to decrease it, right? So you are going to take a negative gradient direction in a small step.
And most of the method we use are usually also called stochastic gradient method. That means that we're not trying to compute a gradient over the entire dataset. Instead, we're trying to take a mini-batch of samples from the dataset, and then we'll try to directly take a small step towards the gradient computed on that mini-batch, and this usually called stochastic gradient method.
Of course, depending on the particular update algorithm the update rule can change. In this particular case, we are trying to show a basic form of stochastic gradient descent. And in the future lectures, we also talk about other forms of update, including things like momentum based method and adaptive gradient method, but in all those cases, the key, and the fundamental problem that we need to solve here is how do we compute this gradient of loss function with respect to theta? And that is the center of almost all the machine learning algorithms
that we use to learn for a given dataset. And of course there are different ways for us to compute this gradient. When I was undergraduate student working on machine learning, the basic skill that we need to learn is how do you manually derive the gradient for a given loss function? Luckily today you no longer have to, because there are a lot of tools that help you to do that automatically.
And to be honest, for the given complexity of models you are dealing with today it's almost infeasible to derive the gradient by hand. So today our goal is trying to talk about some methods that help us to get to those gradient in a more automatic fashion or pragmatic fashion. To begin with, let's start to think about what is the easiest way to compute a gradient.
And when we think about gradient computations, the first thing that we might think about is how do we do that starting from a definition of gradient. If you recall your calculus class, you might remember that a partial differentiation of a function, so in this case, this function f, we are using f to denote the loss function in here, and we're interested in partial derivatives with respect with one our parameters, right? So a gradient of a function theta effectively can be defined as partial derivatives theta_1 f(theta),
partial theta_1 [should Be theta_2] f(theta), partial theta_n f(theta), right? So effectively, if we are able to compute the partial derivative of a function with respect to a single parameter axis then we'll be able to compute the entire gradient. So let's start by remembering our calculus class and trying to find what is the definition of partial gradient.
So this is defined as follows. So for given theta what we're going to do is we're trying to take a small step at the highest direction of my parameter space. So in here, e_i is a unidirectional vector that's zero everywhere and one in one place. And this is the ith element in here. So effectively e_i is a unidirectional vector that points to that particular parameter dimension.
subtracting f(theta), okay? Then we're going to divide it by the step size we're taking. So a partial derivative is defined by taking the limit of this epsilon to zero and the computed difference and divided by the epsilon, okay? So this is a formula that is operationable.
and subtracting that by f(theta), okay? And then divided by epsilon. And if you pick a small enough epsilon that's going to approximately give you the gradient.
subtracting f(theta), we are subtracting f(theta - epsilon e_i).
plus the first order gradient, right? So in this case let's try to do the one dimensional version so it's easier. And plus the two over one f second order gradient delta squared plus O(delta^2), a quadratic term.
So if we plug in delta = epsilon e_i and delta = - epsilon e_i, what we'll find is that this formula effectively is going to have the second term in here canceled out, because regardless whether or not you plug in positive epsilon e_i or minus epsilon e_i, the second order change is always going to be the same.
As a result if I'm going to use this to approximate my gradient, you'll find that the error, the order of errors on the order of O(epsilon^2). While if you use this formula, to approximate our gradient, you will get a higher order return of O(epsilon). So usually if you really want to do numerical computations of gradient, you want to take the second method that gives you a higher and more accurate estimation of the gradient numerically.
So this is the method that actually we can use to compute a gradient, but it is not a method that is commonly used in practice. The reason is that because in a lot of cases, it will suffer from numerical error because in this case we really need epsilon to be small to compute a gradient. The second reason is that remember for each of the i here, we kind of need to run the computation of f(theta + epsilon_i), for each of the i axes here, right? So you have n parameters.
Effectively we need to run two times n evaluations of f. And remember in your neural network, that means we want to run two times n forward evaluations of my function in order to really get a gradient, which is really costly. So usually this is not a formula that's being used in practice because it suffers from numerical error and less efficient compute.
However, it is the formula that's still being very widely used actually in practice for other purposes. The reason that why we want to use this formula is because it's really straightforward to compute, right? So it's really easy to get it right, while if you want to learn other automatic differentiation method while it's okay to still understand the algorithm and you might implement it in some way, and we want to verify that whether or not we implement it correctly.
So numerical differentiation is commonly used to do gradient checking, so it's being used to check if we have an automatic differentiation method that computes a gradient we want to check whether our implementation is correct, okay? However in practice, if we want to do that for each of the theta is, the numerical gradient can still be very costly if we have a lot of input parameters.
So instead we'll use another form that is a bit generalized from the numerical gradient computations that help us to check the gradient correctness. The idea is that instead of trying to pick e_i, we're going to pick a delta function, a delta vector, and this vector, you can pick it randomly from the unit ball.
and compute the right hand side of the formula.
I'm going to check if they are close to each other, okay? So we can do that for a few thetas [deltas] randomly sampled from unit ball, and that sometimes usually sufficient cover all those gradient directions. And it's being a very good test case for us to check, whether our implementation of automatic differentiation is correct.
And you want to really learn this formula because you are going to use it also in your homework. So that before you really submit your homework solutions, you can use this method to check whether or not my gradient computation is correct. And this is also a method being used by most of the deep learning frameworks to check their implementation of automatic differentiation.
Okay, so that's numerical gradient, and numerical gradient checking. Although it's not something that we widely use in practice, it's a very useful tool for us to confirm the correctness of our implementation of gradient computation. So let's move forward to second kind of way to compute a gradient.
And this is kind of a typical way that I will do before automatic differentiation is become popular. So in fact, we want to do is we want to do it by hand. So how do we do it by hand? We're going to go and write down the formula of this f function? And I'm going to try to derive symbolically what the gradient function looks like.
And of course in order to do that I need to apply several gradient derivation rules, right? So including this additional rule that derives what is the gradient, why you want to compute a gradient of sum of two functions, what is the gradient of multiplications of two functions and the chain rule, what is the gradient when you compose function together? So we're apply those rules to derive for a gradient.
equals theta_1 times theta_2 times theta_3 times theta_n, okay? And we're interested in partial derivatives with respect to kth parameter here, theta_k, okay? So in order to do that, what you find is you want to apply the product rule in here, and actually, it's not that.
Yeah, so it's not even a product. You can directly try to derive the partial derivative here if you look at my result is theta_1 times theta_2 times theta_n, and what is the gradient if I want to take gradient with respect to theta_2 the gradient's going to be theta_1 times theta_3 times theta_n, okay? So effectively we want to write it out in a formula.
You'll find that the gradient with respect to theta k, is going to be the product of all the parameters, except theta k. Okay, so same for this particular function. It's really easy for us to go and manually derive the gradient, right? So that is also for a lot of past machine learning works, that's how many people derive their update rule, not necessarily a gradient, but in a similar fashion you want to derive what is the local optimized maximum holding other things to constant, and this kind of gradient derivation is quite common.
with respect to partial theta_k you find that I will need to do n - 2 multiplications, and I need to do that for each other parameters.
That means I need to do n (n - 2), which is a quadratic number of multiplications with respect to the input parameters. However if you want to do that smartly, you'll find out that in order to compute the partial gradient with respect to all the theta, I only need to do about n times multiplications in step.
The reason why this manual symbolic derivation will cost so many duplicate computations is because there are some computations that can be reused among those partial derivatives that are not being reused by simply writing down this symbolic model differentiation formula, okay? So I want everyone to keep that in mind, and you want to keep that question in our head.
We're also going to talk about automatic differentiation methods in the next few slides, and you want to ask, what is the relation between the symbolic differentiation and forward and backward automatic differentiation. There is actually a relation there, and then in certain cases you can derive automatic differentiation method by viewing them as a symbolic differentiation plus some form of simplification.
Okay, so keep it that in mind, and now let's move forward to the first automatic differentiation method. But before we do that, let's start to introduce some basic tools that we're going to use in this class. And if you have used some of deep learning framework before you might have already heard of them called computational graph.
So computational graph is in the center of almost all the machine learning frameworks. And it's also kind of an important tool for us to go and derive automatic differentiation method. So for a computational graph effectively, it is a directed acyclic graph that is used to represent the computation that is being carried out for certain function.
For example, this is a computational graph that represents this particular computation. Of course, for a single computation that could be different ways of computational graph because if you're able to compute the, say multiple of three elements, A, B, and C, you can multiply A and B together then multiply the C, or you can multiply B and C together.
That will correspond to two computational graphs. So computational graph also specifies a bit about the order of execution in certain sense. Each of a node in a computational graph represents a intermediate computation. For example, this particular node represents this log computations that takes in the value of v_1 and outputs value of v_3 that's getting input into another computation v_6.
So each one node represents intermediate value in the computation, and the edge represents input-output relations among those computations. And so each of the computational graph, computation node will have input address that represents input onto them, and their output address represents where their output goes into.
So let's walk through this computational graph on the left hand side, and try to manually evaluate this computational graph. We're going to do this because it's going to be helpful for our future cases on deriving automatic differentiation. So on this particular case, let's assume that we want to, I'm interested in path of two and five, okay? And the way that we're going to evaluate the computational graph is I'm going to place two in here and five in here, and then I'm going to walk this graph
in this topological order. First need to look at v_1. We know that v_1 equals x in here so v_1 = 2, then I'm going to look at v_2 and v_2 = 5, okay? Then I'm going to look at v_3. In this case, v_3 takes v_1 as input and computes a logarithm of the result, and log of two in here, this number I do not try to compute it by hand, but you can always open up a Python or other tools to compute it.
and sine of five equals this number, v_6 is addition of v_3 and v_4, that gives us a result, and finally, v_7 is v_6 subtracting v_5. So that's my final result. Finally, I'm getting a result of y, okay? So it might be a good thing if you have a paper on hand and trying to do it and follow this computational graph trace, you kind of get a sense of what it takes to evaluate the computational graph.
So for a computational graph, what you can do is you can put your value on the input side, and then you're going to walk the computational graph in topological order. For each of the node you try to take the input value, which already computed, because we're walking this in topological order, then we're going to compute a value, output value of that particular node.
And then after we transverse entire graph, you'll be able to get output values on the very end. So this is a tool being used by most of the deep learning frameworks to represent computations. And it's also a very useful tool to derive automatic differentiation, okay? So now we are ready to introduce our first automatic differentiation method and this method is called forward mode automatic differentiation.
So let's assume that we're interested in the partial derivatives of the intermediate value with respect to the first input, in this case x_1 okay? And we're going to recursively derive this value for each of computational graph node. And as the name hints, we want to derive it from the very beginning, okay? So first of all, let's try to see what is v_1 dot in here.
And by definition, we know that we are v_1 dot equals partial v_1, partial x_1, and in this case, we know that v_1 equals x_1, right? So v_1 dot effectively equals in here equals one. So that's why I want to derive from beginning because it's really easy to get those answers, at least for input parameters.
Similarly, v_2 we know v_2 equals x_2, so have nothing to do with x_1. So in this case, v_2 dot equals zero, okay? Now let's look at v_3 dot. We know that v_3 dot equals partial v_3 partial x_1 while v_3 is a function of v_1, right? So it also equals partial log(v_1), partial x_1. And by taking chain rule this will equal v_1 over one times partial v_1, partial x_1, okay? And then you'll find that this gradient value will take two parts, right? First part is actually the partial derivative of v_3 with respect to v_1
and second part equals v_1 dot by definition, so effectively my v_3 dot equals v_1 dot divided by v_1 in here. And if you look at v_1 dot, the value is one, v_1 value is two. So the v_3 dot = 0.5. Okay. Now let's look at v_4. Similarly, we can apply the multiplication rule in here. So we know that v_4 equals v_1 times v_2.
So partial partial x_1, v_1 times v_2, by applying multiplication rule we know that equals partial v_1, partial x_1 times v_2 plus partial v_2, partial x_1 times v_1, okay? And this is v_1 dot, and this is v_2 dot, okay? So that's why we are getting this formula in here. And if we look up the v_1, v_2 values on the left hand side and v_1 dot v_2 dot value here will get the partial derivative of v_4 dot equals five.
Similarly for v_5 that's sine, right? So what we're going to do is the partial derivative of sine is cosine function. That equals v_2 dot times cosine v_2, in this case, zero. And v_6 and v_7 can be derived in the same way. So I would highly encourage everybody to open up a pen, paper, and trying to follow these derivations because it gives you a much better sense of what's going on in this forward mode automatic differentiation.
And finally, you can find that after we walk through each of a computational graph node in the end we're going to get v_7 dot, right? The part of derivative of a final node with respect to the first input. And that is a value that we're getting here. So we can find that why we are calling this forward mode automatic differentiation.
And so the idea is that for each of the step we're trying to starting from our beginning because for the parameters, it's really easy to get those gradient. If the parameter equals the input, the parameter we want to take differentiation with respect to, then the value equals one. Otherwise the value equals zero.
Then for each of the intermediate computational graph node we are trying to reduce the gradient values to the partial derivatives of that node with respect to the input, and the forward mode automatic differentiation value of its input, right? So each step by doing this recursive computations, as long as I know the v_i dot of my input parameters, input node, then we'll be able to compute the v_i dot of this current node and so on.
So after you walk through entire computational graph, you'll get the v_i dot for all them. And in this case, you can also find a computational graph comes in very handy. So you can imagine that how you would want to implement this algorithm using a programing language like Python, okay? So this is forward mode AD, on other hand you'll find that the forward mode AD still comes with a bit of limitations if you want to compute the gradient for like a neural network.
Specifically, if you have a function that takes n inputs and k outputs, a forward mode AD is really good if n is small k is large because it allows us to be able to compute the gradient values with respect to one of the inputs in one pass. So if n is small but I have a lot of outputs in my function, then I will be able to run one pass to be able to get all the gradients of our output with respect to that particular input.
However, if we have n input parameters in order to get a gradient, we'll need to run n forward AD passes to get a gradient with respect to each other input, okay? And in a deep learning case, we mostly care about one output scalars, right? So we're interested in a scalar output where the loss function is a scalar in here and a lot of input parameters.
So we kind of need a different method in order to solve this kind of problem differently. And this is a method that we'll talk about in the second part of today's lecture. It is called reverse mode automatic differentiation. Okay. So let's start to derived the reverse mode automatic differentiation, and if you have a piece of pen, paper by your hand, or if you can get the slides on your side, I would highly recommend you to follow this slide of derivations.
We're going to derive each step by step and try to ask yourself can you really confirm that it is the case in each step of derivations? And this is going to be the majority of content actually for today's lecture, okay? So in order to compute the gradient of a single scalar output value with respect to a lot of inputs, we're going to define a different term called adjoint here, where adjoint is defined as the partial derivative, from the output scalar with respect to each intermediate value node here,
just like forward mode AD, we will be able to derive it in a more recursive fashion. In this case as the name hints we want to derive it from the end of the computational graph as opposed to the beginning because the way that adjoint is being defined, okay? So let's start from the end, let's ask what is adjoint v_7.
In this case by definition, adjoint v_7 is partial y partial v_7. And we know that y is v_7. So we can find that the first step is really easy. The partial derivative, the adjoint value is one in this case, okay? Now we have derived partial, the adjoint value of v_7. Let's look at adjoint value of v_6 here.
And we're interested in partial y, partial v_6. How do we do that? We know that v_7 is a function of v_6, right? So effectively we can apply chain rule in here, and we know that it will equal partial y partial v_7, and then partial v_7 partial v_6, okay? And if you look at the first part, this is the adjoint value of v_7.
The second part is the partial derivative of v_7 with respect to v_6 in this case because v_7 equals v_6 subtracting v_5, that partial derivative is one. So I will be able to derive v_6. So far so good. Let's take a look at v_5 the derivative of v_5 is quite similar because the relation of v_5 and v_6 with respect to v_7 is similar except that the partial derivative of v_7 with respect to v_5 is negative one here, okay? And now let's take like v_4.
In this case, we can do it in a similar fashion as well, because now v_6 is a function of v_4, and y is a function of v_6. So partial y partial v_4 can be written as partial y partial v_6 because v_6 is a function of v_4 multiply partial v_6 partial v_4. And the first part is adjoint value of v_6. Second part is the partial derivative of v_6 with respect to v_4.
In this case equals one, okay? So derivation of v_7 adjoint [should be v_3] can be done in a similar fashion. Things become slightly more interesting if you look at the part, the adjoint value of v_2. And here v_2 is being used by both v_4 and v_5. Okay, so in order to compute the adjoint value of v_2 namely partial y partial v_2 want to be able to take these both pathways into consideration, so it will equal v_5 adjoint times partial derivative of v_5 with respect to v_2.
Intuitively, it's more like a gradient being passed from this pathway. And v_4's adjoint times partial v_4 partial v_2 the gradient being passed back from this pathway. We're going to make a more detailed derivation, a formal derivation in the next slide about why this is the case, okay? And similarly for v_1 it's also been using two pathways.
You can find that there are adjoint values and partial derivatives from two pathways being passed back. So we're going to do a slightly more formal derivation of the two pathway cases in next slide, but let's first take a look at this particular reverse mode AD trace here, okay? We can find that to summarize the idea is that by defining adjoint values, we'll be able to recursively derive the adjoint value of each computational graph node from the very end, so we start from v_7 where the adjoint value is trivial,
because the adjoint value equals one. And then for each of the node, we'll be able to derive the adjoint values from the adjoint value of its next node, of the node that takes this particular one input and partial derivatives of this output node with respect to the input. And after we get the adjoint value of v_1 by definition the adjoint value of v_1 is partial y partial x_1 okay? And adjoint value of v_2 is partial y partial x_2.
And if you put it together, this effectively equals the gradient of f with respect to all the input x parameters here. So that's our gradient, right? So reverse mode AD really allows us to be able to compute the gradient of the scalar function with respect to all the input values in a single backward pass.

Okay? So we have talked about the reverse mode automatic differentiation. Now let's take a closer look at the multiple pathway case. We just glossed over what happens in the multiple pathway case. And why do you want to add those pathways up together? If others tell you like, hey, I learned automatic differentiation, you certainly want to ask a question with this multiple pathway case on how you exactly derive this case where the single input value being used by multiple output values and how do you pass adjoint value?
So let's take this example where v_1 is being used by both v_2 and v_3 okay? So in this particular case, we can actually write the output function y as a function of v_2 and v_3. On the other hand, we also know that v_2 and v_3 both of them are also function of v_1 okay? So in order to take partial derivatives of partial y partial v_1 what we're going to do is we're going to take, so we will need to take partial derivatives of f with respect to each of these input value v_2 and v_3.
So first of all, we're going to take the partial derivative with respect to v_2 holding v_3 as a constant and multiply that by partial v_2 partial v_1 here, then we are going to take the partial derivative value, holding v_2 as a constant, and then multiply that by partial v_3 partial v_1. And finally, we need to add them together to get the final partial derivative.
So this is the formal derivation, so this is kind of like the partial derivative computation rule that you learn in your calculus, so as a result, the final adjoint value of v_1 will become the sum of all those adjoint values of this output times the partial derivatives of the particular output with respect to that input node v_1.
So in this case, actually it might be helpful to also define a notation called partial adjoint of that particular adjoint. So let's say the partial adjoint of i->j here is defined as the adjoint value of output node j and the partial derivatives of the particular output with respect to that input node i.
and partial adjoint of 1->3 together.
And this notation is helpful for us to formally write down the automatic differentiation algorithm and use it in our implementation. Okay, so far we have derived automatic differentiation by hand. You can see that with a computational graph and a definition of adjoint, we'll be able to recursively derive the intermediate gradient in a recursive manner by backward traversing a computational graph.
Now let's start think about how we are going to implement that algorithm using say Python code or using any other languages that you like. So implementation of the algorithm is going to look like this. This is one instance of implementation. Of course, there are other ways to implement it, but this is one way.
So let's walk through this particular implementation, and you want to try to pay attention carefully, because this is also what we are going to implement as the first part of your homework, as a matter of fact it's going to be the foundation of everything we do in this class, okay? So first of all, I'm going to create a dictionary that maps each of the computational graph node to a list of partial adjoint.
So in this case, I'm not trying to map the node to adjoint, but I'm going to map it to a list of partial adjoints so that we can partially accumulate the partial adjoint on the fly and during our computation, okay? So we know that output node adjoint is one from our previous derivations. Then what we can do is reversely traverse my computational graph.
For each of the node i, the first thing I'm going to do, we know that node_to_grad contains a list of partial adjoint. So we're going to sum it together to get my adjoint. Once I get the adjoint value of v_i, what am I going to do is I'm going to compute partial adjoint for each of the inputs to i.
So for each of the k node, from the input list of this particular computational graph node, we're going to compute partial adjoint of v_(k->i), by computing this formula, equals v_i times partial v_i partial v_k. Then we're going to append this partial adjoint to node_to_grad[k] values, and then we're going to repeat this loop in here.
Remember that we are taking these iterations in reverse topological order. That means that after we start to get to node i, we have already visited all the nodes that takes i as an input. That means that the partial adjoint list is already being populated that contains all the partial adjoint values. It allows us to sum everything together in here, right? And finally, after we run everything, we'll be able to get the adjoint of input, and we can return that to the outside, okay? So this is an implementation
partial adjoint here.
A natural answer that one can come up with in this case is I'm just going to use a multidimensional array to store the particular computed result in here, which is a very valid answer. However in practice, we are not actually doing that. Instead, what we're going to do is we're going to run this same algorithm, but we are going to, instead of computing the actual concrete values of each partial adjoint i->j and partial adjoint v_i, we're going to construct a computational graph.
Well, if you don't get what I mean, you will get it once we walk through this example. So here is a computational graph that we are going to take the automatic differentiation with respect to. I'm going to run the code on left hand side. And this particular computational graph express the function exponential of v_2 and this is a plus, plus, sorry.
This particular function is exponential of v_1 plus one times exponential of v_1 in here, right? And in order to compute the reverse mode automatic differentiation, that's going to run this algorithm. The first step we going to compute the partial adjoint where i=4. So I'm going to compute the adjoint value of v_4.
In this case, we know that v_4 adjoint value is one. So instead of trying to directly create an array saying that v_4's value equals one, I'm going to create a computational graph node in here and in this case id is identity function, which means that it's just going to take its input and output it.
So in this case v_4 actually directly equals one. So I'm annotating v_4 bar here so that you can understand that, hey, this is the corresponding node that corresponds to the adjoint value of v_4. As the next step, we are going to come to i=3. And in this case, and in this case, sorry. So in the next step actually, we are going to first get to i=4, but I want to compute the inner loop and want to iterate over all inputs.
equals v_4 bar times v_3.
equals v_4 partial adjoint times v_3. Similarly, the partial adjoint of v_3 in this case, because v_3 only have one output, so the partial joint actually equals adjoint value, equals v_2 times v_4 bar, because again, v_4 equals v_2 times v_3.
exactly equals v_3.
here, right? So after we run this step, what we're going to do is we're going to create a new node
in this case, because v_1 is going to be used by one place, equals v_2 bar times partial v_2 partial v_1.
And because v_2 equals exponential v_1, what is the gradient of exponential function, it's a value of itself, right? So it equals v_2 bar times exponential of v_1. And we also know that exponential v_1 exactly equals this value in here, right? So as a result, the v_1 bar can be expressed as v_2 times v_2 bar in here.
We're kind of saving all of the additional exponential computations by creating this instead of creating another exponential function. Okay, so that's how we run these reverse mode AD computations. And you can find that in the end, unlike the case when we are deriving it by hand, where we are writing down the concrete values of each of our computations, we are creating this computational graph.
It's another computational graph that extends the original computational graph and this new computational graph help us not only contains the forward computation that compute each of the intermediate values, but also contains the gradient computation that computes adjoint value here. One amazing part about this computational graph is unlike the manual derivation case where each of the derivation we can only do it for one specific instance of and x_1=2 and x_2=5.
In this case I can supply any v_1 values here. I can supply v_1 equals zero here, run through this computational graph, and I will be able to get the adjoint value of v_1 when v_1=0. If I supply v_1=2 here, run this computational graph, I will be able to get the adjoint value of v_1 when v_1=2. So this particular computational graph can be reused for different kind of input values and without having to, in an array, run the automatic differentiation derivations from scratch.
So this is just like a few slight interesting advantages that the reverse mode AD algorithm by extending computational graph will bring you. And this is also the algorithm that most of the deep learning framework nowadays uses. You know last lecture we also talk about backpropagation. So one of the question you might have in mind is what is the difference between the reverse mode AD that we just talk about in this lecture versus the backpropagation that we did in the past few lectures.
So roughly they can be summarized in this slide. While we're doing backpropagation, we're constructing the computational graph and we will run the computations forward to get the values. And then when we're interested in gradient, we directly try to run backward operations on the same graph in here.
As a result effectively, we are trying to run back, reversely traverse the original computational graph. There's no extra computational graph node being created during the backward computations. Actually this backpropagation method was the method that the Geoff Hinton originally used and is being used in first generation differing frameworks, namely caffe and cuda-convnet.
When Alex Krizhevsky won the ImageNet challenge and deep learning really takes off, backpropagation was the algorithm that's primarily used in a lot of the machine learning frameworks, except for one which is Theano, which kind of pioneers this reverse mode differentiation idea through computational graph.
So in the second generation, when TensorFlow and PyTorch came out, actually the reverse mode AD by extending computational graph approach takes over. And nowadays most of the deep learning frameworks do not leverage backpropagation algorithm that try to directly do backward operation in place the original computational graph.
Instead, they will always try to construct this new extra gradient part of computation that correspond to adjoint computation. And I want to pause here a bit, and let everybody think a bit of asking this question why. Why does modern deep learning frameworks leverage this reverse mode AD by extending computational graph.
There are several reasons behind this and both in terms of computing gradient, as well as how easy it is to optimize the computation. So first of all, we know that in certain cases, when we're working on machine learning, you might want to do certain loss functions that relates to a gradient. For example, one of a typical loss function that one might ask is I'm interested in minimizing v_1 bar squared in here, okay? So in this case, what we're trying to do is we want to define a loss function, take a gradient with respect to that particular input,
and I want to be able to then take a gradient with respect to a function of that gradient value. So in this case, this is a function of a gradient value in the backpropagation world it's really hard to do that because the backpropagation only defines and tells you how I'm going to do the gradient with respect to the input, right? So it's only going to tell you how do you do the gradient once, but what about gradient of gradient? In the reverse mode AD by extended computational graph though if you take close look on the right,
you'll find that we're taking a computational graph and the result computation is another computational graph. So what we can do in here is we can just attach another node called square that effectively takes v_1 bar squared in here, which is a computational graph. Then we can go and apply the same automatic differentiation algorithm on this computational graph that will gives you the gradient of gradient value with respect to v_1.
So in some sense, you are getting this gradient of gradient for free by building this reverse mode AD by extending computational graph. And this is really cool, and this is a really cool part of this reverse mode AD algorithm of computational graph, and you are going to do it on your homework as well. And the really exciting part is once you implement this automatic differentiation algorithm you kind of get gradient of gradient for free.
The second reason why we typically use this reverse mode AD by extending computational graph approach is that you'll find that output of a gradient graph is still a computational graph. So in order to evaluate this computational graph, all I need to care about is I just need to run this graph forward. As a result, there's more opportunities to go and optimize these computations for example.
In this case, I could fold these two node together and these two node together because they're identity functions that will make the gradient computation asymmetric, there's no longer one-to-one correspondence from the forward computation to the gradient computations, but the gradient computation can become more efficient.
So it also brings a lot more opportunities for underlying machine learning framework to do certain optimizations for the gradient computation. Okay, so up until now, this is the main thing that we focus on this lecture if you want to get one takeaway, just a picture and you really want to understand, making sure you understand the algorithm, how do we get to this reverse mode AD by extending computational graph approach in here.
So up until now we have talked about the automatic differentiation while deriving them using scalar value. In practice when we're dealing with neural networks, all intermediate values are multidimensional tensors. And actually it's not that hard to generalize our reverse mode AD onto multidimensional arrays and tensors.
The way we do that is we generalize the definition of adjoint. So for a matrix like Z, which is a two dimensional matrix or for a tensor value, we're going to define the adjoint of that matrix or tensor by another matrix whose element, each of the element corresponds to the adjoint value with respect to that particular element because the adjoint value with respect to a single scalar is well defined.
As a result, we will be able to use that to define adjoint value of the tensor, right? Then what we can do is we can use a scalar automatic differentiation rule to derive the automatic differentiation rules for these matrix computation and tensor computations. For the forward evaluation we know that Z_ij equals sum of k X_ik times W_kj.
Then we'll be able to derive the adjoint value for particular element, the X_ik in here. And we know that particular X_ik is being used by a lot of Z values, right, by a lot of Z_ij's. So we want to be able to sum those adjoint value and partial derivative together. And finally, we know that equals sum over j W_kj times Z_ij bar in here.
After you derive it in a scalar form, you can write it back on matrix form. And you find that this relation holds where the adjoint value of X equals adjoint value of Z times W transpose. And that is the common rule that we use to be neural network computations, to derive the gradient of a linear layer. Okay, so now the reverse mode AD on tensors can be generalized.
As a result you'll find that the implementation effectively remain the same for tensors and multi-dimensional arrays. We have already talked about the pros and cons of backprop and reverse mode AD, and handling gradient of gradient as well. So finally, this is a bonus slide, we have talked about so far reverse mode AD on tensors.
However, when we're going to write programs, sometimes not all intermediate values are tensors. Sometimes we might want to put tensors in a data structure or other forms. For example, we could have a dictionary where each of a value of the dictionary corresponds to a tensor that we might want to take differentiation with respect to.
We can actually generalize the definition of adjoint onto those data types and data structures. Of course, the definition, you can generalize the definition. For example in this case, for a dictionary we can generalize it by, in this case, we want to make sure the dictionary key itself is always not a tensor.
So it's always a value that we can take differentiation with respect to, but its value can be tensors. So we can define an adjoint as another dictionary that contains the same set of keys, but the value correspond to the corresponding adjoint value. So for each forward computations like dictionary lookup, we'll be able to define a backward computational gradient computations that effectively construct another dictionary that contains the adjoint value.
So the key takeaway here is that for different data types, we will be able to define the adjoint computations as long as we can have a definition for adjoint value and the adjoint propagation rule. The same reverse mode automatic differentiation algorithm should work for those cases. This is kind of a more general case called differentiable programming that we are not going to cover in this lecture and in this class as well.
If you are interested in this concept, you're more than welcome to go and search related literatures on differentiable programming, you'll be able to find some related material here. We may want to have support for certain type of data structure in our assignments through. So specifically we might want to add support for tuple values.
That means that we want to support for a value that contains tuple of multiple tensors. And we want to be able to define adjoint for that particular tuple value. So that will be part of our implementation of the homework, okay? So thanks everybody for coming to today's lecture. In today's lecture we've learned about why do you want to do differentiations and a set of tools for us to be able to both check the gradient of automatic differentiation implementation as well as implementing reverse mode AD
that give us the gradient values of all the inputs in one backward pass. Finally, we also talk about how we can build this reverse mode AD algorithm by extending computational graph so that we can use the same tool to obtain gradient of gradient and getting a lot of benefit on low level optimizations. In the next lecture, we're going to cover more of the implementation details that you are going to use in your homework to implement the algorithm that we described in this lecture.
That's the end of this lecture, thanks for coming. And I will see you in next lecture.
