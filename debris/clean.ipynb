{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMnist(MNISTpath=None, Flat=True, Standardize=False, OneHot=False): # Mnist is already Flat and Normalized(0-1)\n",
    "  import requests, gzip, pickle, os\n",
    "  if MNISTpath is None: MNISTpath = \"/media/moises/D/DLDS\" if os.name == \"posix\" else \"D/DLDS\" # Nix or Win\n",
    "  url = 'https://github.com/mnielsen/neural-networks-and-deep-learning/raw/master/data/mnist.pkl.gz'\n",
    "  if not os.path.exists(os.path.join(MNISTpath, \"mnist.pkl.gz\")):\n",
    "    with open(os.path.join(MNISTpath, \"mnist.pkl.gz\"), \"wb\") as f:\n",
    "      mnistPKLGZ = requests.get(url).content\n",
    "      f.write(mnistPKLGZ)\n",
    "  with gzip.open(os.path.join(MNISTpath, \"mnist.pkl.gz\"), \"rb\") as mn: \n",
    "    (xtr, ytr), (xva, yva), (xte, yte) = pickle.load(mn, encoding=\"latin-1\") # tr, va, te\n",
    "    # ((50000, 784) (50000,)) ((10000, 784) (10000,)) ((10000, 784) (10000,))\n",
    "  if Standardize:\n",
    "    xtr = xtr-xtr.mean(axis=1)[:,None] # Standardizing per-sample (!wrong:change this)\n",
    "    xva = xva-xval.mean(axis=1)[:,None]\n",
    "    xte = xte-xte.mean(axis=1)[:,None]\n",
    "  if OneHot: ytr = one_hot(ytr, 10)\n",
    "  if Flat: return (xtr, ytr), (xva, yva), (xte, yte) # tr, va, te \n",
    "  return (xtr.reshape(-1,1, 28, 28), ytr), (xva.reshape(-1,1, 28, 28), yva), (xte.reshape(-1,1, 28, 28), yte)\n",
    "\n",
    "\n",
    "\n",
    "def shuffle_data(x, y, seed=0):\n",
    "  if seed: np.random.seed(seed)\n",
    "  idx = np.arange(x.shape[0]) # Only Shuffle the highest dim (shape[0])\n",
    "  np.random.shuffle(idx)\n",
    "  return x[idx], y[idx]\n",
    "def one_hot(a, classes=10): return np.eye(classes)[a] # .T Transpose if you don't want torch style\n",
    "def one_hot_vector(a, classes=10): return np.eye(classes)[a].reshape(-1,classes,1) # extra empty dim to work with michs outptu.\n",
    "def accuracy(X,Y): return t.sum(X-Y).item()/ len(X)\n",
    "\n",
    "class Dataset:\n",
    "  def __init__(self, xs, ys, Shuffle:bool=False, OneHot:bool=False, classes:int=None):\n",
    "    if Shuffle: xs, ys = shuffle_data(xs, ys)\n",
    "    self.xs = xs\n",
    "    self.ys = one_hot(ys, classes) if OneHot and not isinstance(classes, int) else ys \n",
    "    self.counter = 0\n",
    "    self.size = xs.shape[0]\n",
    "  def __len__(self): return self.size\n",
    "  def __iter__(self): return self\n",
    "  def __next__(self):\n",
    "    yld = self.xs[self.counter], self.ys[self.counter]\n",
    "    if self.counter < self.size-1: self.counter += 1\n",
    "    else: raise StopIteration\n",
    "    return yld\n",
    "  def __getitem__(self,n): return list(zip(self.xs[n], self.ys[n]))\n",
    "  def __repr__(self): return f\"{self.__class__.__name__}(xs, ys)\"\n",
    "\n",
    "class Batcher:\n",
    "  \"\"\" Batcher is a DS wrapper to iterate over MBS. \n",
    "  Batcher should Shuffle since the DS load data once and batch for many epochs\n",
    "  once the entire DS is exhuasted we create new batcher to reset counter + reshufle \"\"\"\n",
    "  def __init__(self, DS, MBS=128, Shuffle=True): \n",
    "    self.MBS = MBS\n",
    "    self.DS = DS\n",
    "    if Shuffle: DS.xs, DS.ys = shuffle_data(DS.xs, DS.ys)\n",
    "    self.counter = 0\n",
    "    self.size = len(DS)\n",
    "  def __iter__(self): return self\n",
    "  def __repr__(self): return f\"{self.__class__.__name__}(xs, ys, {self.MBS})\"\n",
    "  def __next__(self):\n",
    "    if self.counter >= self.size: raise StopIteration\n",
    "    if self.size>self.counter+self.MBS: \n",
    "      batch = self.DS.xs[self.counter:self.counter+self.MBS], self.DS.ys[self.counter:self.counter+self.MBS]\n",
    "    else: batch = self.DS.xs[self.counter:], self.DS.ys[self.counter:]\n",
    "    self.counter += self.MBS\n",
    "    return batch\n",
    "  def __getitem__(self,n):\n",
    "    if isinstance(n, slice): return list(zip(self.DS.xs[n], self.DS.ys[n]))\n",
    "    elif isinstance(n, int): return self.DS.xs[n], self.DS.ys[n]\n",
    "    else: raise TypeError(f\"Index must be int or slice got {type(n)}\")\n",
    "  def __repr__(self): return f\"{self.__class__.__name__}(xs, ys)\"\n",
    "\n",
    "class MNIST(Dataset):\n",
    "  def __init__(self, Train=True, Validation=False, Flat=True, OneHot=True, **kw):\n",
    "    (trainX, trainY), (valX,ValY), (TestX,TestY) = loadMnist(Flat=Flat, OneHot=OneHot)\n",
    "    if Validation: super().__init__(valX, ValY, **kw)\n",
    "    elif Train: super().__init__(trainX, trainY, **kw)\n",
    "    else: super().__init__(TestX, TestY, **kw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mse(y, p): return 0.5*np.power((y-p), 2).mean()\n",
    "def mseP(y, p): return 2*(p-y)/np.prod(y.shape)\n",
    "\n",
    "def sig(x): return np.reciprocal(1.0+np.exp(-x))\n",
    "def sigP(x): s = sig(x); return s*(1.0-s)\n",
    "def _affTrans(Z, W, B): return Z.dot(W) + B # W(inF,outF) # a = z@w+b -> dL/dz= dL/dz @ w.T \n",
    "def _affTransP(TopGrad, Z, W):\n",
    "    BGrad = TopGrad.sum(axis=0) \n",
    "    WGrad = Z.T.dot(TopGrad) # dL/dw= z.T @ w\n",
    "    Zgrad = TopGrad.dot(W.T) #dL/dz= dL/dz @ w.T\n",
    "    return Zgrad, WGrad, BGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "  def __repr__(self): return self.__class__.__name__\n",
    "  def __call__(self,x ): return self.forward(x)\n",
    "  def forward(self, x): raise NotImplementedError\n",
    "  def backward(self, x): raise NotImplementedError\n",
    "\n",
    "class Sigmoid(Layer): \n",
    "  def forward(self,x): \n",
    "    # print(f\"Forwarding through {self.__class__.__name__}\") \n",
    "    return sig(x)\n",
    "  def backward(self,topGrad): \n",
    "    # print(f\"Backwarding through {self.__class__.__name__} {topGrad.shape}\")\n",
    "    return sigP(topGrad) \n",
    "\n",
    "class Linear(Layer):\n",
    "  def __init__(self,inF, outF): # a = z@w+b -> z[MBS, inF] w[inF, outF] -> [MBS, outF]\n",
    "    self.bias = np.random.randn(outF)\n",
    "    lim = np.sqrt(1/inF)\n",
    "    self.weight = np.random.uniform(-lim, lim, (inF, outF))\n",
    "  def forward(self, x): \n",
    "    # print(f\"Forwarding through {self.__class__.__name__} {self.bias.shape}\")  \n",
    "    self.x = x; return _affTrans(x, self.weight, self.bias)\n",
    "  def backward(self, topGrad, LR=0.1): \n",
    "    # print(f\"backwarding through {self.__class__.__name__}  {self.bias.shape}\")  \n",
    "    zGrad, wGrad, bGrad = _affTransP(topGrad, self.x, self.weight) \n",
    "    self.weight -= wGrad*LR\n",
    "    self.bias -= bGrad*LR\n",
    "    return zGrad\n",
    "\n",
    "class Net:\n",
    "  def __call__(self,x ): return self.forward(x)\n",
    "  def __init__(self):\n",
    "    self.L1 = Linear(784, 100)\n",
    "    self.L2 = Linear(100, 10)\n",
    "    self.Act = Sigmoid()\n",
    "    self.layers = [self.L1, self.Act, self.L2, self.Act]\n",
    "  def forward(self, x):  \n",
    "    for l in self.layers: x = l(x)\n",
    "    return x\n",
    "  def backward(self, topGrad): \n",
    "    for l in reversed(self.layers):  topGrad = l.backward(topGrad)\n",
    "\n",
    "# MBS = 2\n",
    "# MB = np.random.randn(MBS, 784)\n",
    "net = Net()\n",
    "# net(MB).shape # Forward pass is correct\n",
    "# topGrad = np.ones((2, 10))\n",
    "# net.backward(topGrad) # 1.0 if the topGrad of the loss with respect itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784) (10, 10)\n"
     ]
    }
   ],
   "source": [
    "mnist = MNIST(OneHot=True)\n",
    "# MBS = 10\n",
    "# mnistB = Batcher(mnist, MBS)\n",
    "# for x,y in mnistB:\n",
    "#     print(x.shape, y.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_92407/1646038950.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  def sig(x): return np.reciprocal(1.0+np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "MBS = 2\n",
    "topGrad = np.ones((MBS, 10))\n",
    "losses = []\n",
    "net = Net()\n",
    "for i in range(1):\n",
    "    mnistB = Batcher(mnist, MBS)\n",
    "    for x,y in mnistB:\n",
    "        pred = net(x)\n",
    "        loss = mse(pred, y)\n",
    "        losses.append(loss)\n",
    "        topGrad = mseP(y, pred)\n",
    "        net.backward(topGrad)\n",
    "        # print(topGrad.shape)\n",
    "        # break\n",
    "        # net(topGrad.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistB = Batcher(mnist, MBS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(mnistB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_92407/1646038950.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  def sig(x): return np.reciprocal(1.0+np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
