{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TestLinear.py\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from math import ceil, floor\n",
    "as_strided = np.lib.stride_tricks.as_strided\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "  def __init__(self): self.layers_name = self.__class__.__name__\n",
    "  def __call__(self, x): return self.forward(x)\n",
    "  def forward(self, x): raise NotImplementedError\n",
    "  def backward(self, output_gradient, LR): raise NotImplementedError\n",
    "\n",
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def __repr__(self): return self.__class__.__name__\n",
    "    def __call__(self, input): return self.forward(input)\n",
    "    def forward(self, input):\n",
    "        self.input = input # save input for the backward pass\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))\n",
    "\n",
    "\n",
    "# ReLU, LeakyReLU, ELU enable faster and better convergence than sigmoids.\n",
    "# GELU: Gaussian Error Linear Unit used in most Transformers(GPT-3, BERT): paperswithcode.com/method/gelu\n",
    "# Hard-Swish: paperswithcode.com/method/hard-swish\n",
    "\n",
    "def sigmoid(x): return np.reciprocal((1.0+np.exp(-np.clip(x, -100, 2e12))))\n",
    "def sigmoid_prime(x):\n",
    "  s = 1.0/( (1.0+np.exp(-np.clip(x, -100, 2e12))) )\n",
    "  return s * (1 - s) # σ(x)*(1-σ(x))\n",
    "def relu(x): return np.where(x>= 0, x, 0)\n",
    "def relu_prime(x): return np.where(x>= 0, 1, 0)\n",
    "def leaky_relu(x, alpha=0.01): return np.where(x>= 0, x, alpha*x)\n",
    "def leaky_relu_prime(x, alpha=0.01): return np.where(x>= 0, 1, alpha)\n",
    "def elu(x, alpha=0.01): return np.where(x>= 0, x, alpha*(np.exp(x)-1))\n",
    "def elu_prime(x, alpha=0.01): return np.where(x>= 0, 1, alpha*np.exp(x))\n",
    "def swish(x): return x * np.reciprocal((1.0+np.exp(-x))) # x*σ(x) σ(x)+σ'(x)x : σ(x)+σ(x)*(1-σ(x))*x\n",
    "def swish_prime(x): s = np.reciprocal((1.0+np.exp(-x))); return s+s*(1-s)*x #σ(x)+σ(x)*(1-σ(x))*x\n",
    "silu, silu_prime = swish, swish_prime # The SiLU function is also known as the swish function.\n",
    "def tanh(x): return np.tanh(x) # or 2.0*(σ((2.0 * x)))-1.0\n",
    "def tanh_prime(x): return 1 - np.tanh(x) ** 2\n",
    "def gelu(x): return 0.5*x*(1+np.tanh(0.7978845608*(x+0.044715*np.power(x,3)))) # sqrt(2/pi)=0.7978845608\n",
    "def gelu_prime(x): return NotImplemented#Yet Error\n",
    "def quick_gelu(x): return x*sigmoid(x*1.702) # faster version but inacurate\n",
    "def quick_gelu_prime(x): return 1.702*sigmoid_prime(x*1.702)\n",
    "def hardswish(x): return x*relu(x+3.0)/6.0\n",
    "def hardswish_prime(x): return 1.0/6.0 *relu(x+3)*(x+1.0)\n",
    "def softplus(x, limit=20.0, beta=1.0): return (1.0/beta) * np.log(1 + np.exp(x*beta))\n",
    "def softplus_prime(limit=20, beta=1.0): _s = np.exp(x*beta) ; return (beta*_s)/(1+_s)\n",
    "def relu6(x): return relu(x)-relu(x-6)\n",
    "def relu6_prime(x): return relu_prime(x)-relu_prime(x-6)\n",
    "\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "  def __init__(self): super().__init__(sigmoid, sigmoid_prime)\n",
    "class Relu(Activation):\n",
    "  def __init__(self): super().__init__(relu, relu_prime)\n",
    "class Relu6(Activation):\n",
    "  def __init__(self): super().__init__(relu6, relu6_prime)\n",
    "class LeakyRelu(Activation):\n",
    "  def __init__(self, alpha=0.01): super().__init__(leaky_relu, leaky_relu_prime)\n",
    "class Elu(Activation):\n",
    "  def __init__(self, alpha=0.01): super().__init__(elu, elu_prime)\n",
    "class Swish(Activation):\n",
    "  def __init__(self): super().__init__(swish, swish_prime)\n",
    "class Tanh(Activation):\n",
    "  def __init__(self): super().__init__(tanh, tanh_prime)\n",
    "class Gelu(Activation):\n",
    "  def __init__(self): super().__init__(gelu, gelu_prime)\n",
    "class QuickGelu(Activation):\n",
    "  def __init__(self): super().__init__(quick_gelu, quick_gelu_prime)\n",
    "class Hardswish(Activation):\n",
    "  def __init__(self): super().__init__(hardswish, hardswish_prime)\n",
    "class Softplus(Activation):\n",
    "  def __init__(self, limit=20.0, beta=1.0): super().__init__(softplus, softplus_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional\n",
    "\n",
    "def mse(pred, y): return ((pred-y)**2).mean() # $\\sum 1/m (pred-y)^2$\n",
    "def mseP(pred, y): return 2*(pred-y)/np.prod(y.shape) # F(G(x))' should be G(x)' * F(G(x))': (pred-y)*2\n",
    "\n",
    "def _pad(Z: np.ndarray, K: np.ndarray, mode: str=\"valid\") -> np.ndarray:\n",
    "    \"\"\" Check arguments and pad for conv/corr \"\"\"\n",
    "    if mode not in [\"full\", \"same\", \"valid\"]: raise ValueError(\"mode must be one of ['full', 'same', 'valid']\")\n",
    "    if Z.ndim != K.ndim: raise ValueError(\"Z and K must have the same number of dimensions\")\n",
    "    if Z.size == 0 or K.size == 0: raise ValueError(f\"Zero-size arrays not supported in convolutions.\")\n",
    "    ZN,ZC,ZH,ZW = Z.shape\n",
    "    OutCh,KC,KH,KW = K.shape\n",
    "    if ZC!=KC: raise ValueError(f\"Kernel must have the same number channels as Input, got Z.shape:{Z.shape}, W.shape {K.shape}\")\n",
    "    if mode == 'valid' : padding = ((0,0),(0,0), (0,0), (0,0))\n",
    "    elif mode == 'same':\n",
    "        # OH = ZH-KH+1 -> ZH=OH+KH-1\n",
    "        PadTop, PadBottom = floor((KH-1)/2), ceil((KH-1)/2)\n",
    "        PadLeft, PadRigh = floor((KW-1)/2), ceil((KW-1)/2)\n",
    "        padding = ((0,0),(0,0), (PadTop, PadBottom),(PadLeft, PadRigh))\n",
    "    elif mode == 'full':\n",
    "        PadTop, PadBottom = KH-1, KH-1 # full-convolution aligns kernel edge with the firs pixel of input, thus K-1\n",
    "        PadLeft, PadRigh = KW-1, KW-1\n",
    "        padding = ((0,0),(0,0), (PadTop, PadBottom),(PadLeft, PadRigh))\n",
    "    if np.array(padding).any(): Z = np.pad(Z, padding)\n",
    "    return Z, K\n",
    "\n",
    "def _corr2d(Z, W):\n",
    "    Z = Z.transpose(0,2,3,1) # NCHW -> NHWC\n",
    "    W = W.transpose(2,3,1,0) # OIKK -> KKIO\n",
    "\n",
    "    N,ZH,ZW,C_in = Z.shape\n",
    "    KH,KW,_,C_out = W.shape\n",
    "    Ns, ZHs, ZWs, Cs = Z.strides\n",
    "\n",
    "    inner_dim = KH * KW * C_in # Size of kernel flattened\n",
    "    A = as_strided(Z, shape = (N, ZH-KH+1, ZW-KW+1, KH, KW, C_in), strides = (Ns, ZHs, ZWs, ZHs, ZWs, Cs)).reshape(-1,inner_dim)\n",
    "    out = A @ W.reshape(-1, C_out)\n",
    "    return out.reshape(N,ZH-KH+1,ZW-KW+1,C_out).transpose(0,3,1,2) # NHWC -> NCHW\n",
    "\n",
    "def conv2d(Z, W, mode:str=\"valid\"): return _corr2d(*_pad(Z, np.flip(W), mode))\n",
    "def corr2d(Z, W, mode:str=\"valid\"): return _corr2d(*_pad(Z, W, mode))\n",
    "def corr2d_backward(Z, W, TopGrad,  mode:str=\"valid\"):\n",
    "    WGrad = corr2d(Z.transpose(1,0,2,3), TopGrad.transpose(1,0,2,3)).transpose(1,0,2,3)\n",
    "    ZGrad = np.flip(np.rot90(corr2d(TopGrad, W.transpose(1,0,2,3), \"full\"))).transpose(1,0,2,3)\n",
    "    return WGrad , ZGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.9604645e-08\n",
      "-7.450581e-09\n"
     ]
    }
   ],
   "source": [
    "def affTrans(Z, W, B): return Z.dot(W.T) + B # W: (outF,inF)\n",
    "def affTransP(TopGrad, Z, W):\n",
    "    BGrad = TopGrad.sum(axis=0)\n",
    "    WGrad = TopGrad.T.dot(Z)\n",
    "    Zgrad = TopGrad.dot(W)\n",
    "    return Zgrad, WGrad, BGrad\n",
    "\n",
    "class Layer:\n",
    "    \"\"\" All layers should Only acccept batch of inputs: NCHW\"\"\"\n",
    "    def __init__(self): self.trainable = True\n",
    "    def __call__(self, x): return self.forward(x)\n",
    "    def __repr__(self): return f\"{self.layers_name}(Z)\"\n",
    "    def forward(self, input): raise NotImplementedError\n",
    "    def backward(self, TopGrad): raise NotImplementedError\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, inF, outF, bias=True):\n",
    "        self.layers_name = self.__class__.__name__\n",
    "        self.trainable = True\n",
    "        # self.output_shape = N, inF, outF # We must know MBS\n",
    "        lim = 1 / np.sqrt(inF) # Only inF used to calculate the limit, avoid saturation..\n",
    "        self.weight  = np.random.uniform(-lim, lim, (inF, outF))\n",
    "        self.bias = np.random.randn(outF) * 0.1 if bias else None\n",
    "        self.params = [self.weight, self.bias]\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return affTrans(self.input,self.weight, self.bias)\n",
    "\n",
    "    def backward(self, TopGrad):\n",
    "        Zgrad, WGrad, BGrad = affTransP(TopGrad, self.input, self.weight)\n",
    "        self.grads = (WGrad, BGrad)\n",
    "        return Zgrad # Bottom grad\n",
    "\n",
    "MBS = 2\n",
    "inF, outF = 6, 4\n",
    "torch.manual_seed(2)\n",
    "x = torch.rand(MBS,inF)\n",
    "y = torch.rand(MBS,outF)\n",
    "\n",
    "TL = nn.Linear(inF,outF)\n",
    "tpred = TL(x)\n",
    "tloss = F.mse_loss(tpred, y)\n",
    "tloss.backward()\n",
    "\n",
    "\n",
    "ML = Linear(inF,outF)\n",
    "ML.weight, ML.bias = TL.weight.detach().numpy(), TL.bias.detach().numpy() # capy params\n",
    "\n",
    "mpred = ML(x.numpy()) # [MBS,inF] [inF,outF] -> [MBS, outF]\n",
    "mloss = mse(mpred, y.numpy())\n",
    "\n",
    "# #Backward\n",
    "mseGrad = mseP(mpred, y.numpy())\n",
    "mzgrad = ML.backward(mseGrad)\n",
    "print((TL.weight.grad.detach().numpy() - ML.grads[0]).sum())\n",
    "print((TL.bias.grad.detach().numpy() - ML.grads[1]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(Layer):\n",
    "    def __init__(self, inCh, outCh, KS, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        if isinstance(KS, int): KS = (KS, KS)\n",
    "        # if isinstance(stride, int): stride = (stride, stride)\n",
    "        # if isinstance(padding, int): padding = (padding, padding)\n",
    "        # if isinstance(dilation, int): dilation = (dilation, dilation)\n",
    "        # Hout = floor((Hin+2*padding[0]*dilation[0]*(KS[0]−1)−1+1)/stride[0])\n",
    "        # Wout = floor((Win+2*padding[0]*dilation[1]*(KS[1]−1)−1+1)/stride[1])\n",
    "        # self.outShape = outCh, Hout, Wout\n",
    "\n",
    "        self.layers_name = self.__class__.__name__\n",
    "        self.trainable = True\n",
    "        self.weight = np.random.randn(outCh, inCh, *KS) # (outCh,inCh,H,W)\n",
    "        self.bias = np.random.randn(outCh) # Each filter has bias, not each conv window\n",
    "        self.params = [self.weight, self.bias]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return corr2d(x, self.weight) + self.bias[np.newaxis, :, np.newaxis, np.newaxis]\n",
    "\n",
    "    def backward(self, TopGrad):\n",
    "        kernels_gradient, input_gradient = corr2d_backward(self.input, self.weight, TopGrad)\n",
    "        self.grads = (kernels_gradient, TopGrad.sum(axis=(0, 2, 3)))\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1874363e-08"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check conv: \n",
    "MBS, KS = 1, 3\n",
    "inCh, outCh = 1, 1\n",
    "H, W = 4, 4\n",
    "\n",
    "torch.manual_seed(2)\n",
    "x = torch.rand(MBS, inCh, H, W)\n",
    "\n",
    "TC = nn.Conv2d(inCh, outCh, KS)\n",
    "MC = Conv2d(inCh, outCh, KS)\n",
    "MC.weight = TC.weight.detach().numpy()\n",
    "MC.bias = TC.bias.detach().numpy()\n",
    "tpred = TC(x).detach().numpy()\n",
    "mpred = MC(x.numpy())\n",
    "# print((tpred-mpred).mean())\n",
    "\n",
    "tloss = F.mse_loss(tpred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Torch_d_loss_dw = grad(outputs=loss, inputs=w) # compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "MBS, inCh, outCh, H, W = 2, 1, 3, 40,40\n",
    "Z = torch.rand(MBS,inCh,H,W).to(torch.float64)\n",
    "W = torch.rand(outCh,inCh,3,3).to(torch.float64)\n",
    "B = torch.rand(outCh).to(torch.float64)\n",
    "Z.requires_grad_(), W.requires_grad_(), B.requires_grad_()\n",
    "\n",
    "TC = F.conv2d(Z, W, B)\n",
    "TopGrad = torch.ones_like(TC)\n",
    "TWG = grad(TC, W, TopGrad, retain_graph=True)[0]\n",
    "TBG = grad(TC, B, TopGrad, retain_graph=True)[0]\n",
    "TZG = grad(TC, Z, TopGrad, retain_graph=True)[0]\n",
    "\n",
    "MWG, MZG = corr2d_backward(Z.detach().numpy(), W.detach().numpy(), TopGrad.detach().numpy())\n",
    "print((TZG.numpy()-MZG).sum())\n",
    "print((TWG.numpy()-MWG).sum())\n",
    "# print((TBG.numpy()-MBG).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    MBS, inCh, outCh, H, W = np.random.randint(1, 10, 5)\n",
    "    H, W = H+10, W+10\n",
    "    Z = torch.rand(MBS,inCh,H,W).to(torch.float64)\n",
    "    W = torch.rand(outCh,inCh,3,3).to(torch.float64)\n",
    "    B = torch.rand(outCh).to(torch.float64)\n",
    "    Z.requires_grad_(), W.requires_grad_(), B.requires_grad_()\n",
    "    \n",
    "    TC = F.conv2d(Z, W, B)\n",
    "    TopGrad = torch.ones_like(TC)\n",
    "    TWG = grad(TC, W, TopGrad, retain_graph=True)[0]\n",
    "    TBG = grad(TC, B, TopGrad, retain_graph=True)[0]\n",
    "    TZG = grad(TC, Z, TopGrad, retain_graph=True)[0]\n",
    "    \n",
    "    MWG, MZG = corr2d_backward(Z.detach().numpy(), W.detach().numpy(), TopGrad.detach().numpy())\n",
    "    print((TZG.numpy()-MZG).sum())\n",
    "    print((TWG.numpy()-MWG).sum())\n",
    "    # print((TBG.numpy()-MBG).sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
