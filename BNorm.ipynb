{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functional.py\n",
    "\"\"\" Should we keep all functional stuff here includding act/loss ...???\"\"\"\n",
    "import numpy as np\n",
    "from math import ceil, floor\n",
    "as_strided = np.lib.stride_tricks.as_strided\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affTrans(Z, W, B=0): return Z.dot(W.T) + B # W: (outF,inF)\n",
    "def affTransP(TopGrad, Z, W):\n",
    "    BGrad = TopGrad.sum(axis=0)\n",
    "    WGrad = TopGrad.T.dot(Z)\n",
    "    Zgrad = TopGrad.dot(W)\n",
    "    return Zgrad, WGrad, BGrad\n",
    "\n",
    "class Layer:\n",
    "    \"\"\" All layers Only acccept batched input: NCHW\"\"\"\n",
    "    def __call__(self, x): return self.forward(x)\n",
    "    def __repr__(self): return f\"{self.layers_name}(Z)\"\n",
    "    def forward(self, input): raise NotImplementedError\n",
    "    def backward(self, TopGrad): raise NotImplementedError\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, inF, outF, bias=True):\n",
    "        self.layers_name = self.__class__.__name__\n",
    "        self.trainable = True\n",
    "        lim = 1 / np.sqrt(inF) # Only inF used to calculate the limit, avoid saturation..\n",
    "        self.w  = np.random.uniform(-lim, lim, (outF, inF)) # torch style (outF, inF)\n",
    "        self.b = np.random.randn(outF) * 0.1 if bias else None\n",
    "        self.params = (self.w, self.b)\n",
    "        self.inShape, self.outShape = (inF,), (outF,)\n",
    "\n",
    "    def forward(self, z):\n",
    "        self.z = z\n",
    "        return affTrans(self.z, self.w, self.b) # [MBS,inF][outF,inF].T -> [MBS,outF]\n",
    "\n",
    "    def backward(self, TopGrad):\n",
    "        self.zGrad, self.wGrad, self.bGrad = affTransP(TopGrad, self.z, self.w)\n",
    "        return self.zGrad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Layer Weights Gradient:\n",
      "[[ 1.83288932  1.95969429  2.24857789 -1.30090079  5.02126125]\n",
      " [ 4.35974098 -4.12743523 -4.71533437  3.9867369  -2.46517288]\n",
      " [ 3.88826194 -3.4610227  -2.09401212  1.25815586  2.61342896]]\n",
      "PyTorch Weights Gradient:\n",
      "[[ 1.8328893  1.9596944  2.248578  -1.3009008  5.021261 ]\n",
      " [ 4.359741  -4.1274357 -4.715334   3.986737  -2.4651728]\n",
      " [ 3.8882618 -3.4610229 -2.094012   1.258156   2.6134288]]\n",
      "\n",
      "Custom Layer Bias Gradient:\n",
      "[ 0.32418518 -6.70843461  3.43833086]\n",
      "PyTorch Bias Gradient:\n",
      "[ 0.32418537 -6.708435    3.4383307 ]\n",
      "Gradients match!\n"
     ]
    }
   ],
   "source": [
    "# Test function\n",
    "def test_linear_layer(inF, outF, batch_size):\n",
    "    # Create random input and top gradient\n",
    "    Z = np.random.randn(batch_size, inF)\n",
    "    TopGrad = np.random.randn(batch_size, outF)\n",
    "\n",
    "    # Custom Linear Layer\n",
    "    custom_layer = Linear(inF, outF, bias=True)\n",
    "    custom_output = custom_layer.forward(Z)\n",
    "    custom_layer.backward(TopGrad)\n",
    "\n",
    "    # PyTorch Linear Layer\n",
    "    torch_layer = torch.nn.Linear(inF, outF, bias=True)\n",
    "    torch_layer.weight.data = torch.tensor(custom_layer.w, dtype=torch.float32)\n",
    "    torch_layer.bias.data = torch.tensor(custom_layer.b, dtype=torch.float32)\n",
    "\n",
    "    Z_torch = torch.tensor(Z, dtype=torch.float32)\n",
    "    output_torch = torch_layer(Z_torch)\n",
    "\n",
    "    # Create a tensor for TopGrad and perform backpropagation\n",
    "    TopGrad_torch = torch.tensor(TopGrad, dtype=torch.float32)\n",
    "    output_torch.backward(TopGrad_torch)\n",
    "\n",
    "    # Get gradients\n",
    "    torch_w_grad = torch_layer.weight.grad.numpy()\n",
    "    torch_b_grad = torch_layer.bias.grad.numpy()\n",
    "\n",
    "    # Compare gradients\n",
    "    print(\"Custom Layer Weights Gradient:\")\n",
    "    print(custom_layer.wGrad)\n",
    "    print(\"PyTorch Weights Gradient:\")\n",
    "    print(torch_w_grad)\n",
    "\n",
    "    print(\"\\nCustom Layer Bias Gradient:\")\n",
    "    print(custom_layer.bGrad)\n",
    "    print(\"PyTorch Bias Gradient:\")\n",
    "    print(torch_b_grad)\n",
    "\n",
    "    # Check if gradients are close\n",
    "    assert np.allclose(custom_layer.wGrad, torch_w_grad, atol=1e-5), \"Weight gradients do not match!\"\n",
    "    assert np.allclose(custom_layer.bGrad, torch_b_grad, atol=1e-5), \"Bias gradients do not match!\"\n",
    "    print(\"Gradients match!\")\n",
    "\n",
    "# Run the test\n",
    "test_linear_layer(inF=5, outF=3, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm_forward(x, gamma, beta, eps=1e-5):\n",
    "    \"\"\" Forward pass for batch normalization. \"\"\"\n",
    "    # Compute mean and variance for the batch\n",
    "    batch_mean = x.mean(axis=0)\n",
    "    batch_var = x.var(axis=0)\n",
    "\n",
    "    # Normalize the batch\n",
    "    x_normalized = (x - batch_mean) / np.sqrt(batch_var + eps)\n",
    "\n",
    "    # Scale and shift\n",
    "    out = gamma * x_normalized + beta\n",
    "\n",
    "    # Store for backward pass\n",
    "    cache = (x, x_normalized, batch_mean, batch_var, gamma, beta, eps)\n",
    "    return out, cache\n",
    "\n",
    "def batch_norm_backward(d_out, cache):\n",
    "    \"\"\" Backward pass for batch normalization. \"\"\"\n",
    "    x, x_normalized, batch_mean, batch_var, gamma, beta, eps = cache\n",
    "    N, D = x.shape\n",
    "\n",
    "    # Gradient of beta and gamma\n",
    "    dbeta = d_out.sum(axis=0)\n",
    "    dgamma = (d_out * x_normalized).sum(axis=0)\n",
    "\n",
    "    # Gradient of normalized input\n",
    "    d_x_normalized = d_out * gamma\n",
    "\n",
    "    # Gradient of variance and mean\n",
    "    d_var = (d_x_normalized * (x - batch_mean) * -0.5 * (batch_var + eps)**(-1.5)).sum(axis=0)\n",
    "    d_mean = d_x_normalized.sum(axis=0) * -1 / np.sqrt(batch_var + eps) + d_var * -2 * (x - batch_mean).mean(axis=0)\n",
    "\n",
    "    # Gradient of input\n",
    "    dx = d_x_normalized / np.sqrt(batch_var + eps) + d_var * 2 * (x - batch_mean) / N + d_mean / N\n",
    "\n",
    "    return dx, dgamma, dbeta\n",
    "\n",
    "class BatchNorm(Layer):\n",
    "    def __init__(self, num_features, momentum=0.9, eps=1e-5):\n",
    "        self.layers_name = self.__class__.__name__\n",
    "        self.trainable = True\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.gamma = np.ones(num_features)  # Scale\n",
    "        self.beta = np.zeros(num_features)   # Shift\n",
    "\n",
    "        # Running averages for inference\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.training = True  # Flag to switch between training and inference mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            out, self.cache = batch_norm_forward(x, self.gamma, self.beta, self.eps)\n",
    "            # Update running mean and variance\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * x.mean(axis=0)\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * x.var(axis=0)\n",
    "        else:\n",
    "            # During inference, use running mean and variance\n",
    "            x_normalized = (x - self.running_mean) / np.sqrt(self.running_var + self.eps)\n",
    "            out = self.gamma * x_normalized + self.beta\n",
    "        return out\n",
    "\n",
    "    def backward(self, TopGrad):\n",
    "        return batch_norm_backward(TopGrad, self.cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Written for the test\n",
    "\n",
    "# The BatchNorm class implementation as provided\n",
    "def batch_norm_forward(x, gamma, beta, eps=1e-5):\n",
    "    \"\"\" Forward pass for batch normalization. \"\"\"\n",
    "    # Compute mean and variance for the batch\n",
    "    batch_mean = x.mean(axis=0)\n",
    "    batch_var = x.var(axis=0)\n",
    "\n",
    "    # Normalize the batch\n",
    "    x_normalized = (x - batch_mean) / np.sqrt(batch_var + eps)\n",
    "\n",
    "    # Scale and shift\n",
    "    out = gamma * x_normalized + beta\n",
    "\n",
    "    # Store for backward pass\n",
    "    cache = (x, x_normalized, batch_mean, batch_var, gamma, beta, eps)\n",
    "    return out, cache\n",
    "\n",
    "def batch_norm_backward(d_out, cache):\n",
    "    \"\"\" Backward pass for batch normalization. \"\"\"\n",
    "    x, x_normalized, batch_mean, batch_var, gamma, beta, eps = cache\n",
    "    N, D = x.shape\n",
    "\n",
    "    # Gradient of beta and gamma\n",
    "    dbeta = d_out.sum(axis=0)\n",
    "    dgamma = (d_out * x_normalized).sum(axis=0)\n",
    "\n",
    "    # Gradient of normalized input\n",
    "    d_x_normalized = d_out * gamma\n",
    "\n",
    "    # Gradient of variance and mean\n",
    "    d_var = (d_x_normalized * (x - batch_mean) * -0.5 * (batch_var + eps)**(-1.5)).sum(axis=0)\n",
    "    d_mean = d_x_normalized.sum(axis=0) * -1 / np.sqrt(batch_var + eps) + d_var * -2 * (x - batch_mean).mean(axis=0)\n",
    "\n",
    "    # Gradient of input\n",
    "    dx = d_x_normalized / np.sqrt(batch_var + eps) + d_var * 2 * (x - batch_mean) / N + d_mean / N\n",
    "\n",
    "    return dx, dgamma, dbeta\n",
    "\n",
    "class Layer:\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.layers_name}(Z)\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, TopGrad):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class BatchNorm(Layer):\n",
    "    def __init__(self, num_features, momentum=0.9, eps=1e-5):\n",
    "        self.layers_name = self.__class__.__name__\n",
    "        self.trainable = True\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.gamma = np.ones(num_features)  # Scale\n",
    "        self.beta = np.zeros(num_features)   # Shift\n",
    "\n",
    "        # Running averages for inference\n",
    "        self.running_mean = np.zeros(num_features)\n",
    "        self.running_var = np.ones(num_features)\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "        self.training = True  # Flag to switch between training and inference mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            out, self.cache = batch_norm_forward(x, self.gamma, self.beta, self.eps)\n",
    "            # Update running mean and variance\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * x.mean(axis=0)\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * x.var(axis=0)\n",
    "        else:\n",
    "            # During inference, use running mean and variance\n",
    "            x_normalized = (x - self.running_mean) / np.sqrt(self.running_var + self.eps)\n",
    "            out = self.gamma * x_normalized + self.beta\n",
    "        return out\n",
    "\n",
    "    def backward(self, TopGrad):\n",
    "        return batch_norm_backward(TopGrad, self.cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BN = torch.nn.BatchNorm1d(num_features)\n",
    "BN.gamma.data.shape, BN.beta.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test\n",
    "num_features, batch_size = 5, 10\n",
    "\n",
    "# Create random input and top gradient\n",
    "x = np.random.randn(batch_size, num_features)\n",
    "TopGrad = np.random.randn(batch_size, num_features)\n",
    "\n",
    "# Custom BatchNorm Layer\n",
    "custom_bn = BatchNorm(num_features)\n",
    "custom_bn.training = True  # Set to training mode\n",
    "custom_output = custom_bn.forward(x)\n",
    "custom_grad = custom_bn.backward(TopGrad)\n",
    "\n",
    "# PyTorch BatchNorm Layer\n",
    "torch_bn = torch.nn.BatchNorm1d(num_features)\n",
    "torch_bn.weight.data = torch.tensor(custom_bn.gamma, dtype=torch.float32)\n",
    "torch_bn.bias.data = torch.tensor(custom_bn.beta, dtype=torch.float32)\n",
    "torch_bn.running_mean = torch.tensor(custom_bn.running_mean, dtype=torch.float32)\n",
    "torch_bn.running_var = torch.tensor(custom_bn.running_var, dtype=torch.float32)\n",
    "\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "TopGrad_torch = torch.tensor(TopGrad, dtype=torch.float32)\n",
    "\n",
    "# Forward and backward pass in PyTorch\n",
    "torch_output = torch_bn(x_torch)\n",
    "torch_output.backward(TopGrad_torch)\n",
    "\n",
    "# Get gradients\n",
    "torch_gamma_grad = torch_bn.weight.grad.numpy()\n",
    "torch_beta_grad = torch_bn.bias.grad.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outputs and gradients\n",
    "print(\"Custom BatchNorm Output:\")\n",
    "print(custom_output)  # output from custom implementation\n",
    "\n",
    "# print(\"\\nCustom BatchNorm Gradients:\")\n",
    "# print(custom_grad)\n",
    "# print(\"PyTorch BatchNorm Gradients (input):\")\n",
    "# print(torch_bn.weight.grad.numpy())  # gradients from PyTorch for weights\n",
    "\n",
    "# # Check if outputs are close\n",
    "# assert np.allclose(custom_output[0], torch_output.detach().numpy(), atol=1e-5), \"Outputs do not match!\"\n",
    "# # Check if gradients are close\n",
    "# assert np.allclose(custom_grad, torch_bn.weight.grad.numpy(), atol=1e-5), \"Input gradients do not match!\"\n",
    "# print(\"Outputs and gradients match!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sepearator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features, batch_size=5, 10\n",
    "# Create random input and top gradient\n",
    "x = np.random.randn(batch_size, num_features)\n",
    "TopGrad = np.random.randn(batch_size, num_features)\n",
    "\n",
    "# Custom BatchNorm Layer\n",
    "custom_layer = BatchNorm(num_features)\n",
    "custom_output, custom_cache = batch_norm_forward(x, custom_layer.gamma, custom_layer.beta)\n",
    "custom_dx, custom_dgamma, custom_dbeta = batch_norm_backward(TopGrad, custom_cache)\n",
    "\n",
    "# PyTorch BatchNorm Layer\n",
    "torch_layer = torch.nn.BatchNorm1d(num_features, affine=True)\n",
    "torch_layer.weight.data = torch.tensor(custom_layer.gamma, dtype=torch.float32)\n",
    "torch_layer.bias.data = torch.tensor(custom_layer.beta, dtype=torch.float32)\n",
    "\n",
    "x_torch = torch.tensor(x, dtype=torch.float32)\n",
    "output_torch = torch_layer(x_torch)\n",
    "\n",
    "# Create a tensor for TopGrad and perform backpropagation\n",
    "TopGrad_torch = torch.tensor(TopGrad, dtype=torch.float32)\n",
    "output_torch.backward(TopGrad_torch)\n",
    "\n",
    "# # Get gradients\n",
    "# torch_dx = x_torch.grad.numpy()\n",
    "# torch_dgamma = torch_layer.weight.grad.numpy()\n",
    "# torch_dbeta = torch_layer.bias.grad.numpy()\n",
    "\n",
    "# # Compare outputs\n",
    "# print(\"Custom Layer Output:\")\n",
    "# print(custom_output)\n",
    "# print(\"PyTorch Output:\")\n",
    "# print(output_torch.detach().numpy())\n",
    "\n",
    "# # Compare gradients\n",
    "# print(\"\\nCustom Layer Gradients:\")\n",
    "# print(\"dx:\", custom_dx)\n",
    "# print(\"dgamma:\", custom_dgamma)\n",
    "# print(\"dbeta:\", custom_dbeta)\n",
    "\n",
    "# print(\"\\nPyTorch Gradients:\")\n",
    "# print(\"dx:\", torch_dx)\n",
    "# print(\"dgamma:\", torch_dgamma)\n",
    "# print(\"dbeta:\", torch_dbeta)\n",
    "\n",
    "# # Check if outputs and gradients are close\n",
    "# assert np.allclose(custom_output, output_torch.detach().numpy(), atol=1e-5), \"Outputs do not match!\"\n",
    "# assert np.allclose(custom_dx, torch_dx, atol=1e-5), \"dx gradients do not match!\"\n",
    "# assert np.allclose(custom_dgamma, torch_dgamma, atol=1e-5), \"dgamma gradients do not match!\"\n",
    "# assert np.allclose(custom_dbeta, torch_dbeta, atol=1e-5), \"dbeta gradients do not match!\"\n",
    "# print(\"Outputs and gradients match!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test_linear_layer(inF, outF, batch_size):\n",
    "    # Create random input and top gradient\n",
    "    Z = np.random.randn(batch_size, outF)\n",
    "    TopGrad = np.random.randn(batch_size, outF)\n",
    "\n",
    "    # Custom Linear Layer\n",
    "    custom_layer = BatchNorm(outF)\n",
    "    custom_output = custom_layer.forward(Z)\n",
    "    custom_layer.backward(TopGrad)\n",
    "\n",
    "    # PyTorch Linear Layer\n",
    "    torch_layer = torch.nn.Linear(inF, outF, bias=True)\n",
    "    torch_layer.weight.data = torch.tensor(custom_layer.w, dtype=torch.float32)\n",
    "    torch_layer.bias.data = torch.tensor(custom_layer.b, dtype=torch.float32)\n",
    "\n",
    "    Z_torch = torch.tensor(Z, dtype=torch.float32)\n",
    "    output_torch = torch_layer(Z_torch)\n",
    "\n",
    "    # Create a tensor for TopGrad and perform backpropagation\n",
    "    TopGrad_torch = torch.tensor(TopGrad, dtype=torch.float32)\n",
    "    output_torch.backward(TopGrad_torch)\n",
    "\n",
    "    # Get gradients\n",
    "    torch_w_grad = torch_layer.weight.grad.numpy()\n",
    "    torch_b_grad = torch_layer.bias.grad.numpy()\n",
    "\n",
    "    # Compare gradients\n",
    "    print(\"Custom Layer Weights Gradient:\")\n",
    "    print(custom_layer.wGrad)\n",
    "    print(\"PyTorch Weights Gradient:\")\n",
    "    print(torch_w_grad)\n",
    "\n",
    "    print(\"\\nCustom Layer Bias Gradient:\")\n",
    "    print(custom_layer.bGrad)\n",
    "    print(\"PyTorch Bias Gradient:\")\n",
    "    print(torch_b_grad)\n",
    "\n",
    "    # Check if gradients are close\n",
    "    assert np.allclose(custom_layer.wGrad, torch_w_grad, atol=1e-5), \"Weight gradients do not match!\"\n",
    "    assert np.allclose(custom_layer.bGrad, torch_b_grad, atol=1e-5), \"Bias gradients do not match!\"\n",
    "    print(\"Gradients match!\")\n",
    "\n",
    "# Run the test\n",
    "test_linear_layer(inF=5, outF=3, batch_size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
